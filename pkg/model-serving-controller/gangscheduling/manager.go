/*
Copyright The Volcano Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package gangscheduling

import (
	"context"
	"errors"
	"fmt"
	"reflect"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/kubernetes"
	"k8s.io/klog/v2"
	"k8s.io/utils/ptr"

	batchv1alpha1 "volcano.sh/apis/pkg/apis/batch/v1alpha1"
	schedulingv1beta1 "volcano.sh/apis/pkg/apis/scheduling/v1beta1"
	volcanoclient "volcano.sh/apis/pkg/client/clientset/versioned"

	workloadv1alpha1 "github.com/volcano-sh/kthena/pkg/apis/workload/v1alpha1"
	"github.com/volcano-sh/kthena/pkg/model-serving-controller/datastore"
	"github.com/volcano-sh/kthena/pkg/model-serving-controller/utils"
)

// Manager manages PodGroups for gang scheduling
type Manager struct {
	kubeClient    kubernetes.Interface
	volcanoClient volcanoclient.Interface
	store         datastore.Store
}

// NewManager creates a new gang scheduling manager
func NewManager(kubeClient kubernetes.Interface, volcanoClient volcanoclient.Interface, store datastore.Store) Manager {
	return Manager{
		kubeClient:    kubeClient,
		volcanoClient: volcanoClient,
		store:         store,
	}
}

// ManagePodGroups manages PodGroups for a ModelServing instance
func (m *Manager) ManagePodGroups(ctx context.Context, mi *workloadv1alpha1.ModelServing) error {
	if m.isSchedulingEnabled(mi) {
		return m.managePodGroups(ctx, mi)
	}
	return nil
}

// isSchedulingEnabled checks if gang scheduling or networkTopology scheduling is enabled for the ModelServing.
// These advanced scheduling features are only effective when used with the "volcano" scheduler.
func (m *Manager) isSchedulingEnabled(mi *workloadv1alpha1.ModelServing) bool {
	schedulerName := mi.Spec.SchedulerName
	// If schedulerName is empty, Kubernetes uses the default scheduler, which doesn't support gang/network topology.
	isVolcano := schedulerName == "volcano"

	hasGangOrTopology := mi.Spec.Template.GangPolicy != nil || mi.Spec.Template.NetworkTopology != nil

	return isVolcano && hasGangOrTopology
}

// managePodGroups manages PodGroups for group-level gang scheduling
func (m *Manager) managePodGroups(ctx context.Context, mi *workloadv1alpha1.ModelServing) error {
	expectedReplicas := int(*mi.Spec.Replicas)

	// Get existing PodGroups
	existingPodGroups, err := m.getExistingPodGroups(ctx, mi)
	if err != nil {
		return fmt.Errorf("failed to get existing PodGroups: %v", err)
	}

	servingGroupList, err := m.store.GetServingGroupByModelServing(utils.GetNamespaceName(mi))
	if err != nil && !errors.Is(err, datastore.ErrServingGroupNotFound) {
		return fmt.Errorf("failed to get ServingGroups for ModelServing %s: %v", utils.GetNamespaceName(mi), err)
	}

	neededHandledPodGroupNameList := neededHandledPodGroupNameList(expectedReplicas, mi, servingGroupList)
	// Create or update PodGroups for each ServingGroup
	for _, podGroupName := range neededHandledPodGroupNameList {
		// podGroupName := m.generatePodGroupName(mi.Name, i)

		if existingPG, exists := existingPodGroups[podGroupName]; exists {
			// Update existing PodGroup if needed
			if err := m.updatePodGroupIfNeeded(ctx, existingPG, mi); err != nil {
				return fmt.Errorf("failed to update PodGroup %s: %v", podGroupName, err)
			}
		} else {
			// Create new PodGroup
			if err := m.createPodGroup(ctx, mi, podGroupName); err != nil {
				return fmt.Errorf("failed to create PodGroup %s: %v", podGroupName, err)
			}
		}
	}

	// Clean up excess PodGroups
	// As insurance
	return m.cleanupExcessPodGroups(ctx, mi, existingPodGroups, expectedReplicas)
}

// createPodGroup creates a PodGroup for group-level gang scheduling
func (m *Manager) createPodGroup(ctx context.Context, mi *workloadv1alpha1.ModelServing, podGroupName string) error {
	// podGroupName := m.generatePodGroupName(mi.Name, groupIndex)

	// Calculate total pods and resources for this ServingGroup
	minMember, minTaskMember, minResources := m.calculateRequirements(mi, podGroupName)

	podGroup := &schedulingv1beta1.PodGroup{
		ObjectMeta: metav1.ObjectMeta{
			Name:      podGroupName,
			Namespace: mi.Namespace,
			Labels: map[string]string{
				workloadv1alpha1.ModelServingNameLabelKey: mi.Name,
				workloadv1alpha1.GroupNameLabelKey:        podGroupName,
			},
			Annotations: map[string]string{
				schedulingv1beta1.KubeGroupNameAnnotationKey: podGroupName,
			},
			OwnerReferences: m.buildOwnerReference(mi),
		},
		Spec: schedulingv1beta1.PodGroupSpec{
			MinMember:     int32(minMember),
			MinTaskMember: minTaskMember,
			MinResources:  &minResources,
		},
	}

	podGroup = appendNetworkTopologyPolicy(mi, podGroup)

	_, err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).Create(ctx, podGroup, metav1.CreateOptions{})
	if err != nil && !apierrors.IsAlreadyExists(err) {
		return err
	}

	klog.V(2).Infof("Created PodGroup %s for group-level gang scheduling", podGroupName)
	return nil
}

// To build ownerReferences of PodGroup
func (m *Manager) buildOwnerReference(mi *workloadv1alpha1.ModelServing) []metav1.OwnerReference {
	return []metav1.OwnerReference{
		{
			APIVersion: workloadv1alpha1.GroupVersion.String(),
			Kind:       workloadv1alpha1.ModelServingKind.Kind,
			Name:       mi.Name,
			UID:        mi.UID,
			Controller: ptr.To(true),
		},
	}
}

// calculateRequirements calculates requirements for role-level gang scheduling
func (m *Manager) calculateRequirements(mi *workloadv1alpha1.ModelServing, podGroupName string) (int, map[string]int32, corev1.ResourceList) {
	minMember := 0
	minTaskMember := make(map[string]int32)
	minResources := corev1.ResourceList{}

	// For role-level, only include roles up to MinRoleReplicas limit
	for _, role := range mi.Spec.Template.Roles {
		roleReplicas := int(*role.Replicas)
		minRoleReplicas := roleReplicas // Default to all replicas

		if mi.Spec.Template.GangPolicy.MinRoleReplicas != nil {
			if minReplicas, exists := mi.Spec.Template.GangPolicy.MinRoleReplicas[role.Name]; exists {
				minRoleReplicas = int(minReplicas)
			}
		}

		expectReplicas := min(minRoleReplicas, roleReplicas)
		roleList, err := m.store.GetRoleList(utils.GetNamespaceName(mi), podGroupName, role.Name)
		if err != nil {
			klog.V(2).Infof("Failed to get role list for role %s: %v", role.Name, err)
		}
		// During scaling operations, podGroup does not affect scaling policies.
		// Under the binpack scaling strategy, it is unknown which role replicas will be deleted.
		// Therefore, no action is taken during scaling.
		// PodGroup will updated after the role completes scaling down.
		if len(roleList) > expectReplicas {
			continue
		}
		// When length(roleList) <= expectReplicas, that is, when scaling up or updating.
		// Provide the roleNameList to be updated.
		needHandledRoleNameList := needHandledRoleNameList(expectReplicas, roleList, role.Name)

		// Only include role replicas up to the minimum required
		// for roleIndex := 0; roleIndex < minRoleReplicas && roleIndex < roleReplicas; roleIndex++ {
		for _, taskName := range needHandledRoleNameList {
			// taskName := m.GenerateTaskName(role.Name, roleIndex)
			podsPerTask := 1 + int(role.WorkerReplicas) // entry + workers
			minTaskMember[taskName] = int32(podsPerTask)
			minMember += podsPerTask

			// Aggregate resources
			m.aggregateResources(&minResources, &role.EntryTemplate.Spec)
			if role.WorkerTemplate != nil {
				for i := 0; i < int(role.WorkerReplicas); i++ {
					m.aggregateResources(&minResources, &role.WorkerTemplate.Spec)
				}
			}
		}
	}
	fmt.Printf("minMember: %d, minTaskMember: %v, minResources: %v\n", minMember, minTaskMember, minResources)
	return minMember, minTaskMember, minResources
}

// aggregateResources aggregates resource requirements from a pod spec
func (m *Manager) aggregateResources(total *corev1.ResourceList, podSpec *corev1.PodSpec) {
	if *total == nil {
		*total = corev1.ResourceList{}
	}

	for _, container := range podSpec.Containers {
		for resourceName, quantity := range container.Resources.Requests {
			if existing, exists := (*total)[resourceName]; exists {
				existing.Add(quantity)
				(*total)[resourceName] = existing
			} else {
				(*total)[resourceName] = quantity.DeepCopy()
			}
		}
	}
}

// generatePodGroupName generates PodGroup name for group-level scheduling
func (m *Manager) generatePodGroupName(modelServingName string, groupIndex int) string {
	return fmt.Sprintf("%s-%d", modelServingName, groupIndex)
}

// GenerateTaskName generates task name for MinTaskMember
func (m *Manager) GenerateTaskName(roleName string, roleIndex int) string {
	return fmt.Sprintf("%s-%d", roleName, roleIndex)
}

// getExistingPodGroups gets existing PodGroups for a ModelServing
func (m *Manager) getExistingPodGroups(ctx context.Context, mi *workloadv1alpha1.ModelServing) (map[string]*schedulingv1beta1.PodGroup, error) {
	selector := labels.SelectorFromSet(map[string]string{
		workloadv1alpha1.ModelServingNameLabelKey: mi.Name,
	})

	podGroupList, err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).List(ctx, metav1.ListOptions{
		LabelSelector: selector.String(),
	})
	if err != nil {
		return nil, err
	}

	result := make(map[string]*schedulingv1beta1.PodGroup)
	for i := range podGroupList.Items {
		pg := &podGroupList.Items[i]
		result[pg.Name] = pg
	}

	return result, nil
}

// updatePodGroupIfNeeded updates a PodGroup if needed for group-level scheduling
func (m *Manager) updatePodGroupIfNeeded(ctx context.Context, existing *schedulingv1beta1.PodGroup, mi *workloadv1alpha1.ModelServing) error {
	// Calculate current requirements
	minMember, minTaskMember, minResources := m.calculateRequirements(mi, existing.GetName())

	updated := existing.DeepCopy()
	updated.Spec.MinMember = int32(minMember)
	updated.Spec.MinTaskMember = minTaskMember
	updated.Spec.MinResources = &minResources

	// Apply network topology policy
	updated = appendNetworkTopologyPolicy(mi, updated)

	if hasPodGroupChanged(existing, updated) {
		_, err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).Update(ctx, updated, metav1.UpdateOptions{})
		if err != nil {
			return err
		}
		klog.V(2).Infof("Updated PodGroup %s for group-level gang scheduling", existing.Name)
	}

	return nil
}

func (m *Manager) DeletePodGroupWhenServingGroupDeleted(ctx context.Context, mi *workloadv1alpha1.ModelServing, servingGroupName string) error {
	if err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).Delete(ctx, servingGroupName, metav1.DeleteOptions{}); err != nil {
		if !apierrors.IsNotFound(err) {
			return err
		}
	}
	return nil
}

// cleanupExcessPodGroups cleans up excess PodGroups
func (m *Manager) cleanupExcessPodGroups(ctx context.Context, mi *workloadv1alpha1.ModelServing, existingPodGroups map[string]*schedulingv1beta1.PodGroup, expectedReplicas int) error {
	for podGroupName, podGroup := range existingPodGroups {
		// Check if this PodGroup is still needed
		isNeeded := false
		for i := 0; i < expectedReplicas; i++ {
			expectedName := m.generatePodGroupName(mi.Name, i)
			if podGroupName == expectedName {
				isNeeded = true
				break
			}
		}

		if !isNeeded {
			err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).Delete(ctx, podGroup.Name, metav1.DeleteOptions{})
			if err != nil && !apierrors.IsNotFound(err) {
				return fmt.Errorf("failed to delete excess PodGroup %s: %v", podGroup.Name, err)
			}
			klog.V(2).Infof("Deleted excess PodGroup %s", podGroup.Name)
		}
	}

	return nil
}

// cleanupPodGroups cleans up all PodGroups for a ModelServing
func (m *Manager) CleanupPodGroups(ctx context.Context, mi *workloadv1alpha1.ModelServing) error {
	existingPodGroups, err := m.getExistingPodGroups(ctx, mi)
	if err != nil {
		return fmt.Errorf("failed to get existing PodGroups for cleanup: %v", err)
	}

	for _, podGroup := range existingPodGroups {
		err := m.volcanoClient.SchedulingV1beta1().PodGroups(mi.Namespace).Delete(ctx, podGroup.Name, metav1.DeleteOptions{})
		if err != nil && !apierrors.IsNotFound(err) {
			return fmt.Errorf("failed to delete PodGroup %s: %v", podGroup.Name, err)
		}
		klog.V(2).Infof("Deleted PodGroup %s (gang scheduling disabled)", podGroup.Name)
	}

	return nil
}

// AnnotatePodWithPodGroup annotates a pod with the appropriate PodGroup information
func (m *Manager) AnnotatePodWithPodGroup(pod *corev1.Pod, mi *workloadv1alpha1.ModelServing, minMember int, groupName, taskName string) {
	if !m.isSchedulingEnabled(mi) {
		return
	}

	if pod.Annotations == nil {
		pod.Annotations = make(map[string]string)
	}

	// Add volcano annotation
	pod.Annotations[schedulingv1beta1.KubeGroupNameAnnotationKey] = groupName
	pod.Annotations[batchv1alpha1.TaskSpecKey] = taskName
}

func hasPodGroupChanged(current, updated *schedulingv1beta1.PodGroup) bool {
	return current.Spec.MinMember != updated.Spec.MinMember ||
		!reflect.DeepEqual(current.Spec.MinTaskMember, updated.Spec.MinTaskMember) ||
		!reflect.DeepEqual(current.Spec.MinResources, updated.Spec.MinResources) ||
		!reflect.DeepEqual(current.Spec.NetworkTopology, updated.Spec.NetworkTopology) ||
		!reflect.DeepEqual(current.Spec.SubGroupPolicy, updated.Spec.SubGroupPolicy)
}

// neededHandlerPodGroupNameList returns the list of PodGroup names that need to be handled
func neededHandledPodGroupNameList(expectedReplicas int, mi *workloadv1alpha1.ModelServing, servingGroupNameList []datastore.ServingGroup) []string {
	// Changes to the PodGroup will not affect Pods that have already been deployed.
	// During binpack scale down, it is unknown which ServingGroup will be deleted.
	// Therefore, return all podGroup names that exist.
	// Deletion of PodGroups is handled when ServingGroups are deleted.
	podGroupNameListlength := max(expectedReplicas, len(servingGroupNameList))

	nameList := make([]string, 0, podGroupNameListlength)
	for _, group := range servingGroupNameList {
		_, index := utils.GetParentNameAndOrdinal(group.Name)
		if index > podGroupNameListlength-1 {
			nameList = append(nameList, group.Name)
			podGroupNameListlength = podGroupNameListlength - 1
		}
	}

	for i := 0; i < podGroupNameListlength; i++ {
		nameList = append(nameList, utils.GenerateServingGroupName(mi.GetName(), i))
	}
	return nameList
}

// NeedHandledRoleNameList is used in Role scale up scenario to get the roleName list that need scale up.
// Therefore, the default value for `expectedReplicas` is greater than `length(RoleList)`.
// Or the Role update scenario. (This scenario is This scenario is relatively rare. Since it is not permitted to modify an already configured gangPolicy,
// and in practical applications, the workerReplicas within a deployed role are rarely altered.)
func needHandledRoleNameList(expectedReplicas int, existRoleList []datastore.Role, roleName string) []string {
	scaleUpRoleNameList := make([]string, 0, expectedReplicas)

	for _, role := range existRoleList {
		_, index := utils.GetParentNameAndOrdinal(role.Name)
		if index > expectedReplicas-1 {
			scaleUpRoleNameList = append(scaleUpRoleNameList, role.Name)
			expectedReplicas = expectedReplicas - 1
		}
	}

	for i := 0; i < expectedReplicas; i++ {
		scaleUpRoleNameList = append(scaleUpRoleNameList, utils.GenerateRoleID(roleName, i))
	}
	return scaleUpRoleNameList
}

// equalSubGroupNetworkTopology compares two volcano SubGroupPolicySpec pointers for equality
func equalSubGroupNetworkTopology(a []schedulingv1beta1.SubGroupPolicySpec, b *schedulingv1beta1.NetworkTopologySpec) bool {
	if len(a) == 0 && b == nil {
		return true
	}

	if len(a) == 0 || b == nil {
		return false
	}

	if a[0].MatchPolicy == nil {
		return false
	}

	if len(a[0].MatchPolicy) < 2 || a[0].MatchPolicy[0].LabelKey != workloadv1alpha1.RoleLabelKey ||
		a[0].MatchPolicy[1].LabelKey != workloadv1alpha1.RoleIDKey {
		return false
	}

	if a[0].NetworkTopology == nil {
		return false
	}
	// The podGroup.SubGroupPolicy created by modelServing has a length of 1, so only the first element needs to be compared
	return a[0].NetworkTopology.Mode == b.Mode &&
		a[0].NetworkTopology.HighestTierAllowed == b.HighestTierAllowed
}

func appendNetworkTopologyPolicy(mi *workloadv1alpha1.ModelServing, podGroup *schedulingv1beta1.PodGroup) *schedulingv1beta1.PodGroup {
	if mi.Spec.Template.NetworkTopology != nil {
		// set NetworkTopology if configured in ModelServing
		if mi.Spec.Template.NetworkTopology.GroupPolicy != nil {
			podGroup.Spec.NetworkTopology = mi.Spec.Template.NetworkTopology.GroupPolicy
		}

		// set SubGroupPolicy if configured in ModelServing
		if mi.Spec.Template.NetworkTopology.RolePolicy != nil {
			podGroup.Spec.SubGroupPolicy = []schedulingv1beta1.SubGroupPolicySpec{
				{
					Name:            podGroup.GetName(),
					NetworkTopology: mi.Spec.Template.NetworkTopology.RolePolicy,
					MatchPolicy: []schedulingv1beta1.MatchPolicySpec{
						{
							LabelKey: workloadv1alpha1.RoleLabelKey,
						},
						{
							LabelKey: workloadv1alpha1.RoleIDKey,
						},
					},
				},
			}
		}
	}
	return podGroup
}
